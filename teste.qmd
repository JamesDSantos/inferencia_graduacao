---
title: "Testes de hipóteses"
---


## Definição de hipóteses, testes e modos de avaliação

Existem diversos problemas nos quais o objetivo da inferência é levantar evidências de alguma suposição sobre a população $F(.)$. Tais suposições são denominadas hipóteses.

<div class='alert laert-success'>
**Definição.** Qualquer suposição sobre $F(.)$ é denominada 
</div>

<div class='alert laert-info'>
**Exemplo.**
Seja $X_1,\ldots,X_n$ uma amostra de variáveis aleatórias. 
Ao menos que existam razões físicas claras, considerar que estas variáveis são independentes e identicamente distribuídas é uma hipótese.
</div>

Podem existir diferentes hipóteses para o mesmo problema. É comum identificar a $i$-ésima hipótese por $H_i$, com $i=0,1,2,\ldots$. Seguem alguns exemplos:

* $H_1:\theta=0$.
* $H_2:\theta>0$.
* $H_3:$ $X_1,\ldots,X_n\sim\hbox{Normal}(\mu,\sigma^2)$ para algum par $(\mu,\sigma^2)$ desconhecido.
* $H_4:$ $X_1,\ldots,X_n$ é uma amostra de variáveis aleatórias independentes.
* $H_5:$ $X_1,\ldots,X_n$ é uma amostra de variáveis aleatórias independentes com distribuição normal.

Nesse curso, será discutida apenas a abordagem paramétrica. Sob essa ótica, hipóteses são suposições sobre os parâmetros da população, o que implica em hipóteses do tipo $$H_i:\theta\in \Theta_I\subset\Theta.$$ Nesta abordagem, existem dois tipos importantes de hipóteses:
* Hipóteses simples: são do tipo $H:\theta=\theta_0$. Exemplos: $H_0:\theta=\theta_0$; $H_1:\{\alpha=\alpha_0\}\cap\{\beta=\beta_0\}$. Uma hipótese simples identifica completamente a população.
* Hipóteses compostas: são do tipo $H:\theta\in \Theta'\subset \Theta$. Exemplos: $H_0:\theta\leq \theta_0$; $H_1:\{\alpha\leq\alpha_0\}\cup\{\beta\geq\beta_0\}$. Observe que essas hipóteses trazem muitas possibilidades para o valor de $\theta$, tornando impossível determinar completamente a população.

Os testes de hipóteses são procedimentos que utilizam uma amostra para decidir se certa hipótese é verdadeira ou falsa. 
Ao considerar uma certa hipótese $H_0:\theta\in\Theta_0$,  onde $\Theta_0\subset \Theta$, após observar os dados existem duas decisões:
* Decisão 1: aceitar a hipótese $H_0$ como verdadeira.
* Decisão 2: aceitar a hipótese $H_0$ como falsa.
Note que estas decisões são estatísticas, uma vez que elas são baseadas na amostra.

<div class='alert alert-success'>
**Teste de Hipóteses.** 
Um  teste de hipóteses (também chamado de regra de decisão) é qualquer estatística $D:\mathcal{X}^n\rightarrow \{0,1\}$. Se
$D(\textbf{x})=1$, toma-se a decisão de rejeitar $H_0$ e se $D(\textbf{x})=0$, toma-se a decisão de não rejeitar $H_0$. 
</div>


Para racionalizar o processo de decisão, seja

$$\delta=\left\{\begin{array}{ll}1,& \hbox{ se $H_0$ é falsa }\\ 0,&\hbox{ se $H_0$ é verdadeira}\end{array}\right.$$
Observe que o teste $D$ pode ser considerado um estimador para $\delta$. Deste modo, a qualidade do teste $D$ pode ser verificada através do erro quadrático médio:

$$\begin{align}EQM_{D}(\delta)&=E(D-\delta)^2=\delta^2P(D=0|\delta)+(1-\delta)^2P(D=1|\delta)\\
&= \delta^2(1-P(D=1|\delta))+(1-\delta)^2 P(D=1|\delta)\\
&=\delta^2+[-\delta^2+(1-\delta)^2]P(D=1|\delta)\\
&=\delta^2+[1-2\delta]P(D=1|\delta)\end{align}$$

Observe que o erro quadrático médio depende de $P(D=1|\delta)$, que por sua vez só é conhecido quando $\delta$ é conhecido. Ao ver essa probabilidade como função de $\delta$, temos a função poder do teste (posteriormente definiremos apropriadamente essa função).

Considere os seguintes cenários:

* Se $\delta=0$ (a hipótese é verdadeira), teremos 

$$EQM_D(0)=P(D=1|\delta=0)$$
e quanto menor o valor do poder do teste, menor é o erro quadrático médio. É imediato que essa probabilidade se refere à ação de rejeitar, erroneamente, a hipótese $H_0$ quando ela é verdadeira. Esse erro é denominado **erro do tipo I**.

* Se $\delta=1$ ( a hipótese é falsa), então
$$EQM_D(1)=1-P(D=1|\delta=1),$$
e, nesse caso, quanto maior o valor do poder do teste, menor é o erro quadrático médio. Obviamente, $1-P(D=1|\delta=1)=P(D=0|\delta=1)$. Portanto, o erro quadrático médio é definido como a probabilidade de não rejeitar (erroneamente) a hipótese $H_0$ quando ela é falsa. Esse erro é denominado **erro do tipo II.** 


<div class='alert alert-warning'>
**Importante!** Os erros do tipo I e II não são complementares, uma vez que o primeiro é calculado sob $\delta=0$ e o segundo sob $\delta=1$. Contudo, há uma relação importante entre eles, ilustrada nos cenários abaixo:

1. Suponha que o teste $D$ nunca comete o erro do tipo I. Então 
$P(D=1|\delta=0)=0$. Isso só pode acontecer se a decisão $D=1$ nunca for realizada. Logo, quando $\delta=1$, 
$$1-P(D=1|\delta=1)=P(D=0|\delta=1)=1$$
e o erro do tipo II será cometido com probabilidade 1.

2. Alternativamente, suponha que o teste $D$ nunca comete o erro do tipo II. Então 
$P(D=0|\delta=1)=0$. Isso só pode acontecer se a decisão $D=0$ nunca for realizada. Logo, quando $\delta=0$, 
$$0=P(D=0|\delta=0)=1-P(D=1|\delta=0)\Rightarrow P(D=1|\delta=0)=1$$
e o erro do tipo I será cometido com probabilidade 1.
</div>

Observe que não é possível eliminar a ocorrência de um dos erros (tipo I ou II) sem correr o risco de sempre cometer o outro erro. Para ter algum controle sobre o problema, escolhemos a probabilidade de cometer o erro tipo I para ficar controlada. Tal probabilidade é denominada nível de significância.

<div class='alert alert-success'>
**Definição** Dizemos que o teste $D$ tem nível de significância $\alpha$ se a probabilidade de cometer o erro do tipo I é menor ou igual à $\alpha$. 
</div>

Uma vez que decidimos controlar o erro do tipo I, mantendo o nível de significância em $\alpha$,  sabemos que nosso teste $D$ vai cometer o erro de rejeitar $H_0$ com probabilidade $\alpha$ sempre que $H_0$ for verdadeira. Como apenas esse cenário está controlado, é comum formular a hipótese $H_0$ como algo que **deve ser rejeitado** (daí vem o termo "hipótese nula" para ser referir à $H_0$).

<div class='alert alert-info'>
**Exemplo.** Dizemos que certa máquina está desregulada se sua probabilidade ($\theta$) de produzir itens defeituosos é maior que $0,001$. Um lote de peças produzidos por essa máquina é separado e cada peça é testada. O objetivo é saber se a máquina está regulada, ou seja, se a atual probabilidade é menor que $0,001$. Recorde que é usual definir a hipótese nula como o complementar do que queremos testar, o que gera
$$H_0:\theta\geq 0,001\;\; (\hbox{ máquina desregulada})$$

Vejamos os riscos envolvidos nesse processo de decisão:

* Erro tipo I: dizemos que a máquina está operando normalmente, quando na verdade ela está desregulada. Os lotes serão vendidos, os clientes serão prejudicados, a marca vai perder credibilidade, pode ser necessário fazer o recall dos lotes produzidos.

* Erro tipo II: dizemos que a máquina está desregulada, quando na verdade ela está operando normalmente. A linha de produção deve ser interrompida e a equipe técnica tem que avaliar a máquina. O problema se resolve dentro da própria empresa.

Para testar essa hipóte, escolhemos um teste  $D$ com nível de significância de 1%. Ao observar a amostra, duas coisas podem acontecer:

* $D(\textbf{x})=1$. Nesse caso, assumimos que a máquina está operando normalmente. Tomamos essa decisão porque a probabilidade do teste errar nessa situação é de 1%

* $D(\textbf{x})=0$. Nesse caso, assumimos que a máquina está desregulada. A probabilidade do erro tipo II em geral é maior que o nível de significância (no nosso caso 1%). Ainda sim, cometer esse erro implica em acionar a equipe técnica para uma avaliação, algo muito menos problemático que o erro do tipo I.

</div>


<div class='alert alert-warning'>
**Importante!** Embora $D(x)=0$ seja a decisão de aceitar $H_0$ como verdadeira, alguns textos utilizam termos como "não há evidências para rejeitar $H_0$", ou "o teste falhou em rejeitar $H_0$". O motivo por trás desses termos está o fato de que o erro tipo II não está controlado, logo, não sabemos com que frequência a decisão de aceitar a hipótese vai estar equivocada. 
</div>

## A construção de um teste de hipóteses a partir de uma estatística de teste

Para construir um teste de hipóteses, é preciso particionar o espaço amostral de duas regiões:

* Região de rejeição: são todas as amostras que levam à decisão de rejeitar a hipótese nula.

* Região de aceitação: são todas as amostras que levam à decisão de aceitar a hipótese nula.

Pode ser uma tarefa árdua descobrir como particionar o espaço amostral de maneira adequada para obter um teste de hipóteses. Conforme já discutido no Capítulo 3, podemos utilizar uma estatística $T$ para reduzir a dimensão do problema. Uma vez que o teste $D$ e a estatística $T$ são funções da amostra, podemos definir um teste com a composição $D(T(\textbf{x}))$. Quando utilizada dessa forma, $T$ é denominada *estatística de teste*. A figura abaixo ilustra esse conceito.

![A estatística $T$ utilizada como estatística de teste. As partições em verde formam as regiões de rejeição e as partições laranjas as de aceitação.  ](fig_stat_test.jpg){#fig::stat_test}

Em geral, escolhemos uma estatística de teste que possui alguma relação com $\theta$ (como, por exemplo, uma estatística suficiente ou um estimador). Tal opção é útil para definir como a partição será feita. Para ilustrar,  considere que $T$ é um estimador para $\theta$ e suponha que desejamos testar se $H_0:\theta\in \Theta_0$. Como $T$ é um estimador para $\theta$, pelo princípio das substituição, é razoável inferir que não se deve rejeitar $H_0$ se $T\in\Theta_0.$ Nas próximas 3 figuras, discutimos melhor essas ideias. Nelaas, mostramos o espaço paramétrico $\Theta$ e o espaço da hipótese nula $\Theta_0$ (região em salmão). O ponto em laranja é o verdadeiro valor do parâmetro. Em tons diferentes de verde, temos a densidade da estatística $T$, sendo que quanto mais escura, maior é a probabilidade de observar $t$ naquela região. Temos as seguintes situações:

* Na figura abaixo, a distribuição da estatística de teste gera somente valores dentro de $\Theta_0$. Portanto, sempre vamos inferir que $\theta\in\Theta_0$ e é razoável aceitar $H_0$.

![Nesse exemplo, os valores de $T$ sempre são gerados dentro do espaço da hipótese nula.  ](fig_testA.jpg){#fig::stat_testB}


* Na figura abaixo, a distribuição da estatística de teste nunca gera valores dentro de $\Theta_0$. Portanto, sempre vamos inferir que $\theta\notin\Theta_0$ e rejeitamos $H_0$.

![Nesse exemplo, os valores de $T$ nunca são gerados dentro do espaço da hipótese nula.  ](fig_testB.jpg){#fig::stat_testB}


* Nesta figura, a hipótese ainda é verdadeira, mas existe a possibilidade da estatística de teste gerar valores fora do espaço da hipótese nula. É nessa situação que é possível cometer o erro tipo I.

![Aqui, a hipótese nula é verdadeira, mas é possível observar $t\notin\Theta_0$.  ](fig_testc.jpg){#fig::stat_testC}

* A discussão realizada na última figura foi feita para um único ponto na borda de $\Theta_0$. Como não sabemos qual ponto é o correto, devemos levar em consideração a borda toda, conforme mostra a figura abaixo. Qualquer ponto $t$ gerado na região mais verde claro pode ter sido gerado supondo que a hipótese nula é verdadeira.

![Considerando todas as possibilidades para a hipótesen ula, valores de $t$ dentro de $\Theta_0$ ou na área verde podem ser gerados quando $H_0$ é verdadeira.  ](fig_testD.jpg){#fig::stat_testD}

* Agora, podemos particionar o espaço, criando uma região de rejeição e outra de aceitação. Como sempre devemos ter uma probabilidade baixa de cometer o erro tipo I, devemos colocar parte da área verde dentro da região de rejeição. Na figura abaixo, aceitamos $H_0$ quando $t$ está dentro do círculo vermelho e rejeitamos em caso contrário.

![Partição do espaço. A área dentro do círculo vermelho é a região de aceitação, que compreende a região $\Theta_0$ e a maioria dos valores de $T$ que podem ser gerados quando $H_0$ é verdadeira. A área fora do círculo vermelho inclui os pontos impossíveis e alguns pontos pouco prováveis de serem gerados por quando $H_0$ é verdadeira.](fig_testE.jpg){#fig::stat_testD}




<div class='alert alert-info'>
**Exemplo.** Seja $X_1,\ldots,X_{n}$ uma amostra aleatória da população Normal$(\mu,1)$. Suponha que desejamos testar $H_0:\mu=\mu_0$. 

Sabemos que $\bar{X}_{n}$ é um estimador para $\mu$. Se $\bar{x}_{n}=\mu_0$, não temos motivos para rejeitar $H_0$. Abaixo, mostramos a distribuição de $\bar{X}_{n}\sim N(\mu_0,1/n)$. É possível notar que para qualquer valor de $\bar{x}_{n}$ muito distantes, onde a cauda da normal é praticamente é praticamente nula, nos leva a rejeitar $H_0$ (uma vez que é improvável que esses valores sejam gerados se $H_0$ fosse verdadeira). Naturalmente, devemos escolher dois pontos, $a$ e $b$ tais que rejeitaremos $H_0$ sempre que $\bar{x}_{n}<a$ ou $\bar{x}_n>b$.

```{r echo = FALSE}
oo <- par(cex = 1.3)
plot.new()
plot.window( xlim=c(-5,5), ylim=c(0,.4))
polygon( c(seq(-5,-1.96,length=10),seq(-1.96,-5,length=10)), c(dnorm(seq(-5,-1.96,length=10)), rep(0,10)),col='gray')
polygon( c(seq(5,1.96,length=10),seq(1.96,5,length=10)), c(dnorm(seq(5,1.96,length=10)), rep(0,10)),col='gray')
curve(dnorm(x), add=T,lwd =2)
axis(1,at=c(-5,-1.96,0,1.96,5), labels=c('','a',expression(mu[0]),'b',''))

par(oo)
```

Como precisamos ter chance de cometer o erro tipo I, fixamos o nível de significância em $\alpha$ e em seguida escolhemos dois pontos $a,b$ ($a<b$) tais que

$$\alpha=P(\bar{X}_n\leq a\hbox{ ou }\bar{X}_n\geq b|\mu_0,1/n)=P(\bar{X}_n\leq a|\mu_0,1/n)+P(\bar{X}_n\geq b|\mu_0,1/n)$$

Fazendo 
$$\frac{\alpha}{2}=P(\bar{X}_n\leq a|\mu_0,1/n),$$
teremos que $a$ é o quantil $\alpha/2$ da distribuição Normal($\mu_0,1/n$). Fazendo 
$$\frac{\alpha}{2}=P(\bar{X}_n\geq b|\mu_0,1/n)\Rightarrow \frac{\alpha}{2}=P(\bar{X}_n\leq b|\mu_0,1/n)=1-\frac{\alpha}{2}$$
teremos que $b$ é o quantil $1-\alpha/2$ da distribuição Normal($\mu_0,1/n$). Portanto, nosso teste de hipóteses é:

$$D(\bar{x}_n)=\left\{\begin{array}{ll}1,&\hbox{ se \bar{x}_n<a\hbox{ ou }\bar{x}_n>b} \\
0,&\hbox{ se a\leq \bar{x}_n \leq b}\end{array}\right.$$
</div>
<div class='alert alert-info'>
**Exemplo.** Um agricultor afirma que a média de produção de soja em sua fazenda maior que 3 toneladas por hectare. Uma amostra de 15 hectares foi selecionada e a produção de soja média foi de 3,2 toneladas. Supondo que a produção média de soja por hectare tem distribuição normal com desvio padrão de 0,5 tonelada por hectare, teste, ao nível de 5% de significância, a afirmação do agricultor.

**Solução.** Queremos saber se $\mu>3$. Portanto, a hipótese nula é
$$H_0:\mu\leq 3.$$
Vamos utilizar o estimador $\bar{x}_{15}$. A figura abaixo aprenta várias possibilidade para a distribuição de $\bar{X}_{15}$ quando $H_0$ é verdadeira, sendo que a curva mais à direita corresponde à distribuição normal com média $\mu=3$. Os pontos verdes representam valores que podem ocorrer quando a hipótese nula é verdadeira. Já os pontos vermelhos ocorrem com uma probabilidade muito baixa quando $H_0$ é verdadeira. Nosso objetivo é encontrar o ponto $c$ que vai particionar os valores de $\bar{x}_{15}$, criando as regiões de rejeição e aceitação.

::: {#fig}
::: {.figure-content}
```{r echo = FALSE}
plot.new()
plot.window( xlim=c(-3,12), ylim=c(0,.4) )
polygon( c(seq(qnorm(.95,3),12,length=15),
           seq(12,qnorm(.95,3),length=15)),
         c( dnorm(seq(qnorm(.95,3),12,length=15),3),  rep(0,15)), col = 'salmon' )
curve( dnorm(x,3,1), add =T, lwd = 2)
curve( dnorm(x,2,1), add =T, lty = 2)
curve( dnorm(x,1,1), add =T, lty = 2)
curve( dnorm(x,0,1), add =T, lty = 2)
axis(1,at=c(-3,3,qnorm(.95,3,1),12), labels=c('',3,'c',''))
title( xlab= expression(bar(x)[15]))
points(-3,0, pch=16, col = 'green4')
points(-2,0, pch=16, col = 'green4')
points(-.5,0, pch=16, col = 'green4')
points(-1,0, pch=16, col = 'green4')
points(.4,0, pch=16, col = 'green4')
points(1.4,0, pch=16, col = 'green4')
points(3,0, pch=16, col = 'green4')
points(3.2,0, pch=16, col = 'green4')
abline(v=qnorm(.95,3,1))
#points(qnorm(.95,3,1),0, pch=16, col = 'red')
points(qnorm(.95,3,1)+.5,0, pch=16, col = 'red')
points(qnorm(.95,3,1)+2,0, pch=16, col = 'red')
points(qnorm(.95,3,1)+3,0, pch=16, col = 'red')
points(qnorm(.95,3,1)+4,0, pch=16, col = 'red')


```
:::
As funções densidade acima são apenas algumas das infinitas possibilidades de distribuição para $\bar{X}_{15}$ quando $H_0:\mu\leq 3$. 
:::


Utilizando $\bar{x}_{15}$ para tomar a decisão, teremos a seguinte região de rejeição:
$$R=\{\bar{x}_{15}:\bar{x}_{15}>c\}.$$
Para determinar o valor de $c$, observe que
$$0,05=P(\bar{X}_{15}>c|\mu=3)=1-P(\bar{X}_{15}\leq c|\mu=3)\Rightarrow F_{\bar{X}_{15}|\mu=3}(c)=0,95$$
logo $c$ é o quantil de 95% da distribuição $$\bar{X}_{15}|\mu=3\sim N\left(3,\frac{0,5^2}{15}\right)=N\left(3,\frac{1}{60}\right)$$
O valor de $c=3,212$. Portanto, rejeitamos $H_0$ quando $\bar{x}_{15}>3,212$. Como $\bar{x}_{15}=3,2$, não rejeitamos a hipótese nula.
</div>



<div class='alert alert-info'>
**Exemplo.** O número de chamadas em um call center possui distribuição Poisson com taxa 120 chamadas. Após a implementação de uma nova campanha de marketing, a empresa deseja verificar se a taxa média de chamadas aumentou. Para isso, foi coletada uma nova amostra de dados, correspondente a 100 horas de atendimento, resultando em uma média de 135 chamadas por hora. Teste, ao nível de significância de 5%, se a nova campanha teve efeito.

**Solução.** Deseja-se saber se houve aumento na taxa de chamadas, ou seja, se $\theta>120$. Portanto, a hipótese nula pode ser formulada como $H_0:\theta\leq 120$.

Considerando o estimador de máxima verossimilhança como estatística de teste, rejeitamos a hipótese nula quando
$\hat{\theta}>c$, com $c>120$. Fixando o nível de significância em 5%, teremos que
$$0,05=P(\hat{\theta}>c|\theta=120)=1-P(\hat{\theta}\leq c|\theta=120)$$
e, como
$$P(\hat{\theta}\leq c|\theta)=P\left(\sum_{i=1}^{100}X_i\leq 100c|\theta=120\right)=F_T(100c),$$
onde $T\sim\hbox{Poisson}(12000)$. Unindo as duas equações acima, concluímos que,
$$F_T(100c)=0,95$$
ou seja $100c$ é o quantil 95% da distribuição Poisson(12000). Abaixo apresentamos o cálculo de $c$ no `R`:

```{r}
c = qpois(.95,12000)/100
c
```
Portanto, a região de rejeição desse teste é $$R=\{\hat{\theta}:\hat{\theta}>121,8\}.$$
Como o valor observado de $\hat{\theta}$ é $135$, rejeitamos a hipótese nula. Portanto, há evidências de que a campanha teve efeito.
</div>


## O teste da razão de verossimilhanças

Considere a $H_0:\theta\in\Theta_0\subset \Theta$. Seja $\hat{\theta}_0$, o valor em $\Theta_0$ tal que
$$\begin{equation}
L(\theta)\leq L(\hat{\theta}_0),
\end{equation}$$

para todo $\theta\in\Theta_0$. O valor $\hat{\theta}_0$ pode ser interpretado como sendo a estimativa de máxima verossimilhança de $\theta$ quando a hipótese $H_0$ é verdadeira.

Agora, seja $\hat{\theta}$ o EMV para $\theta$. Se $L(\hat{\theta}_0)$ estiver próximo do valor de $L(\hat{\theta})$, então o valor mais verossímil de $\Theta_0$ está próximo do valor mais verossímil de $\Theta$, dando evidências de que $H_0$ é verdadeira. Portanto, valores pequenos da estatística 
$$\begin{align}
\lambda(\textbf{X})=\frac{L(\hat{\theta}_0)}{L(\hat{\theta})},
\end{align}$$
dão evidências de que $H_0$ é falsa.

::: {#fig}

::: {.figure-content}
```{r echo = FALSE}
oo <- par( mfrow = c(1,2), cex = 1.2)
plot.new()
plot.window( xlim = c(-3,3), ylim=c(0,.4))
polygon( c(seq(-3,-1,length =20 ),seq(-1,-3,length=20)),
          c(dnorm(seq(-3,-1,length =20 )), rep(0,20)), col='gray')                            
curve(dnorm(x), add= T, lwd = 2)
axis(1, at=c(-3,-1,0,3), 
labels=c('',expression(theta[0]),expression(hat(theta)),''))
axis(2, at=c(0,.4),labels = c('',''))
title( xlab= expression(theta), ylab=expression(L(theta)))

plot.new()
plot.window( xlim = c(-3,3), ylim=c(0,.4))
polygon( c(seq(-3,1,length =20 ),seq(1,-3,length=20)),
          c(dnorm(seq(-3,1,length =20 )), rep(0,20)), col='gray')                            
curve(dnorm(x), add= T, lwd = 2)
axis(1, at=c(-3,1,0,3), 
labels=c('',expression(theta[0]),expression(hat(theta)),''))
axis(2, at=c(0,.4),labels = c('',''))
title( xlab= expression(theta), ylab=expression(L(theta)))

par(oo)
```
:::
Figura. A região cinza corresponde ao conjunto da hipótese nula. No gráfico da esquerda, o ponto mais verossímil da hipótese nula está afastado da estimativa de máxima verossimilhança, levando à rejeição de $H_0$. Já no gráfico seguinte, a estimativa de máxima verossimilhança está dentro do conjunto da hipótese nula e, portanto, não hà motivos para rejeitá-la.
:::

<div class='alert alert-success'>
**Definição** Considere a hipótese $H_0:\theta\in\Theta_0\subset \Theta$. O teste para esta hipótese que utiliza a estatística 
$$\lambda(\textbf{X})=\frac{L(\hat{\theta}_0)}{L(\hat{\theta})}$$ com região de rejeição dada por
$$R=\{ \lambda(\textbf{x}): \lambda(\textbf{x})\leq k \}$$ para algum valor de $0<k<1$ fixado é denominado Teste da Razão de Verossimilhanças (TRV). O valor de $k$ é escolhido de modo que o teste tenha nível de significância $\alpha$, ou seja, $k$ satisfaz
$$ P(\lambda(\textbf{X})<k|\theta)\leq \alpha,$$
para todo $\theta\in\Theta_0$.
</div>


<div class='alert alert-info'>
**Exemplo.** Seja $X_1,\ldots,X_n$ uma amostra aleatória da população Normal$(\theta,1)$ e considere a hipótese $H_0:\theta=\theta_0$. Vamos encontrar o teste da razão de verossimilhanças. Para tanto, devemos primeiro encontrar os estimadores de máxima verossimilhança $\hat{\theta}$ e $\hat{\theta}_0$.

Já sabemos que o estimador de máxima verossimilhança $\hat{\theta}$ é $\bar{X}_n$. Como $\Theta_0=\{\theta_0\}$, temos que $\hat{\theta}_0=\theta_0$.

A estatística do teste da razão de verossimilhanças é
$$\begin{align}\lambda(\textbf{x})&=\frac{L(\hat{\theta}_0)}{L(\hat{\theta})}=\frac{\left(\frac{1}{2\pi}\right)^{\frac{n}{2}}\exp\left\{-\frac{1}{2}\sum_{i=1}^n(x_i-\mu_0)^2\right\}}{\left(\frac{1}{2\pi}\right)^{\frac{n}{2}}\exp\left\{-\frac{1}{2}\sum_{i=1}^n(x_i-\bar{x})^2\right\}}\\&=\frac{\exp\left\{-\frac{1}{2}\sum_{i=1}^n(x_i^2+\mu_0^2 -2\mu_0x_i)\right\}}{\exp\left\{-\frac{1}{2}\sum_{i=1}^n(x_i^2+\bar{x}^2-2x_i\bar{x})\right\}}\\&=\exp\left\{-\frac{1}{2}\left[n\mu_0^2 -2\mu_0\sum_{i=1}^n x_i\right]+\frac{1}{2}\left[n\bar{x}^2-2\bar{x}\sum_{i=1}^n x_i\right] \right\}\\&=\exp\left\{-\frac{1}{2}\left[n\mu_0^2-2\mu_0 n\bar{x}+n\bar{x}^2\right] \right\}\\&=\exp\left\{-\frac{n}{2}(\bar{x}-\mu_0)^2\right\}\end{align}$$
Decidimos rejeitar $H_0$ se $\lambda(\textbf{x})<k$, onde $k$ é o valor que satisfaz
$$P(\lambda(\textbf{X})<k|\theta_0)=\alpha.$$
Observe que
$$\lambda(\textbf{x})<k\Rightarrow e^{-\frac{n}{2}(\bar{x}-\mu_0)^2}<k$$
Na figura abaixo é possível observar que existem $a$ e $b$ tais que $\lambda(x)<k$ se e somente se $\bar{x}<a$ ou $\bar{x}>b$.

```{r echo = F}
oo <- par( cex = 1.2)
plot.new()
plot.window( xlim=c(-3,3), ylim=c(0,.4))
curve(dnorm(x,0,1), lwd = 2, add =T)
xx <- seq(-3,-1.64,length.out = 20)
xx <- c(xx,xx[20:1])
yy <- c( dnorm(xx[1:20]),rep(0,20))
polygon(xx,yy,col='gray')
xx <- seq(3,1.64,length.out = 20)
xx <- c(xx,xx[20:1])
yy <- c( dnorm(xx[1:20]),rep(0,20))
polygon(xx,yy,col='gray')

axis(1, at=c(-3,-1.64,0,1.64,3),labels = c('','a',expression( mu[0]),'b',''))
axis(2,at = c(0,dnorm(1.64),.4), labels= c('','k',''))
abline(h=dnorm(1.64), lwd =2)
title( xlab = expression(bar(x)), ylab=expression(lambda(x)))
par(oo)
```
Portanto, o teste da razão de verossimilhanças pode ser escrito como
$$D=\left\{\begin{array}{ll}1,&\hbox{ se }\bar{x}<a\hbox{ ou } \bar{x}>b\\ 0&, \hbox{se }\bar{x}\in[a,b]\end{array}\right.$$
onde $a$ e $b$ são os quantis $\alpha/2$ e $1-\alpha/2$ da distribuição Normal($\mu_0,1/n$). 
</div>


<div class='alert alert-info'>
**Exemplo (O teste t).** Seja $X_1,\ldots,X_n$ uma amostra aleatória da população Normal($\mu,\sigma^2$) e considere da hipótese $H_0:\mu=\mu_0$. Sabemos que os estimadores de máxima verossimilhança para $\theta=(\mu,\sigma^2)$ são $\hat{\mu}=\bar{X}_n$ e $$\hat{\sigma}^2=\frac{n-1}{n}S_2=\frac{1}{n}\sum_{i=1}^n (X_i-\bar{X}_n)^2.$$
Para construir o teste da razão de verossimilhanças, pecisamos primeiro encontrar os estimadores de maxima verossimilhança considerando que a hipótese nula é verdadeira. Nesse caso, temos que $\hat{\mu}_0=\mu_0$ e 

$$\hat{\sigma}^2_0=\frac{1}{n}\sum_{i=1}^n (X_i-\mu_0)^2.$$
A estatística do teste da razão de verossimilhanças é dada por
$$\begin{align}\lambda(\textbf{x})&=\frac{L(\hat{\theta}_0)}{L(\hat{\theta})}=\frac{\left(\frac{1}{2\pi\hat{\sigma}^2_0}\right)^{\frac{n}{2}} e^{-\frac{1}{2\hat{\sigma}^2_0}\sum_{i=1}^n(x_i-\mu_0)^2}}{\left(\frac{1}{2\pi\hat{\sigma}^2}\right)^{\frac{n}{2}} e^{-\frac{1}{2\hat{\sigma}^2}\sum_{i=1}^n(x_i-\bar{x}_n)^2}}\\&=\left(\frac{\hat{\sigma}^2}{\hat{\sigma}^2_0}\right)^{\frac{n}{2}}
\end{align}$$
Observe que
$$\begin{align}\hat{\sigma}^2_0&=\frac{1}{n}\sum_{i=1}^n (x_i-\bar{x}_n-\mu_0+\bar{x}_n)^2\\&=\frac{1}{n}\sum_{i=1}^n (x_i-\bar{x}_n)^2 +(\bar{x}_n+\mu_0)^2=\hat{\sigma}^2+(\bar{x}_n-\mu_0)^2\end{align}$$
então
$$\lambda(\textbf{x})=\left(\frac{\hat{\sigma}^2}{\hat{\sigma}^2_0}\right)^{\frac{n}{2}}=\left(\frac{\hat{\sigma}^2}{\hat{\sigma}^2+(\bar{x}_n-\mu_0)^2}\right)^{\frac{n}{2}}=\left(\frac{n}{n+n\frac{(\bar{x}_n-\mu_0)^2}{\hat{\sigma}^2}}\right)^{\frac{n}{2}}$$
e, fazendo
$$t^2=n\frac{(\bar{x}_n-\mu_0)^2}{\hat{\sigma}^2},$$
teremos 
$$\lambda(\textbf{x})=\left(\frac{n}{n+t^2}\right)^{\frac{n}{2}}$$
A figura abaixo mostra o gráfico de $\lambda$ como função de $t$. Observe que, existe $a$ tal que, $\lambda(\textbf{x})<k$ se e somente se $t<-a$ ou $t>a$.
```{r echo = FALSE}
oo <- par( cex = 1.2)
FUNa <- function(t) (10/(10+t^2))^5
FUN <- function(x) sapply(x, FUNa)
plot.new()
plot.window( xlim=c(-4,4), ylim=c(0,1))
curve(FUN(x), lwd = 2, add =T)
xx <- seq(-3,-1.64,length.out = 20)
xx <- c(xx,xx[20:1])
yy <- c( FUN(xx[1:20]),rep(0,20))
polygon(xx,yy,col='gray')
xx <- seq(3,1.64,length.out = 20)
xx <- c(xx,xx[20:1])
yy <- c( FUN(xx[1:20]),rep(0,20))
polygon(xx,yy,col='gray')

axis(1, at=c(-3,-1.64,0,1.64,3),labels = c('','-a',0,'a',''))
axis(2,at = c(0,dnorm(1.64),1), labels= c('','k',''))
abline(h=dnorm(1.64), lwd =2)
title( xlab = expression(bar(x)), ylab=expression(lambda(x)))
par(oo)
```
</div>

<div class='alert alert-info'>
**Exemplo (diferença entre médias)** Sejam $X_1,\ldots,X_n$ e $Y_1,\ldots,Y_m$ duas amostras aleatórias independentes com populações Normal($\mu_X$,$\sigma^2$) e Normal($\mu_Y$,$\sigma^2$), respectivamente. Considere a hipótese  $H_0:\mu_X=\mu_Y$. Vamos encontrar o teste da razão de verossimilhanças para esta hipótese.

Primeiro, vamos encontrar os estimadores de máxima verossimilhança. A função de verossimilhança é dada por

$$\begin{align}L(\mu_X,\mu_Y,\sigma^2)&=\prod_{i=1}^n f(x_i|\mu_X,\sigma^2)\prod_{j=1}^m f(y_i|\mu_Y,\sigma^2)\\ &=\left(\frac{1}{2\pi\sigma^2}\right)^{\frac{n}{2}}e^{-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(x_i-\mu_X)^2}\left(\frac{1}{2\pi\sigma^2}\right)^{\frac{m}{2}}e^{-\frac{1}{2\sigma^2}\sum_{j=1}^m(y_j-\mu_Y)^2}\\&=\left(\frac{1}{2\pi\sigma^2}\right)^{\frac{n+m}{2}}e^{-\frac{1}{2\sigma^2}\left[\sum_{i=1}^n(x_i-\mu_X)^2+\sum_{j=1}^m(y_j-\mu_Y)^2\right]}\end{align}$$
e seu logaritmo é
$$\log L(\mu_X,\mu_Y,\sigma^2)=-\frac{n+m}{2}\log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\left[\sum_{i=1}^n(x_i-\mu_X)^2+\sum_{j=1}^m(y_j-\mu_Y)^2\right]$$
As derivadas parciais são
$$\begin{align}\frac{\partial}{\partial\mu_X}\log L(\mu_X,\mu_Y,\sigma^2)&=\frac{1}{\sigma^2}\sum_{i=1}^n(x_i-\mu_X)\\
\frac{\partial}{\partial\mu_Y}\log L(\mu_X,\mu_Y,\sigma^2)&=\frac{1}{\sigma^2}\sum_{j=1}^m(y_j-\mu_Y)\\
\frac{\partial}{\partial\sigma^2}\log L(\mu_X,\mu_Y,\sigma^2)&=-\frac{n+m}{2\sigma^2}+\frac{1}{2\sigma^4}\left[\sum_{i=1}^n(x_i-\mu_X)^2+\sum_{j=1}^m(y_j-\mu_Y)^2\right]
\end{align}$$
É imediato que os pontos críticos são
$$\begin{align}
\hat{\mu}_X&=\bar{x}_n\\
\hat{\mu}_Y&=\bar{y}_m\\
\hat{\sigma}^2&=\frac{1}{n+m}\left[\sum_{i=1}^n(x_i-\bar{x}_n)^2+\sum_{j=1}^m(y_j-\bar{y}_m)^2\right]
\end{align}$$
Pode-se mostrar que os pontos críticos são pontos de máximo. 

Agora, supondo que $H_0$ é verdadeira, teremos que $\mu_X=\mu_Y=\mu_0$. Deste modo, $X_1,\ldots,X_n,Y_1,\ldots,Y_m$ representa uma amostra aleatória da população Normal$(\mu_0,\sigma^2)$ e os estimadores de máxima verossimilhança são
$$\begin{align}
\hat{\mu}_0&=\frac{1}{n+m}\left[\sum_{i=1}^n X_i + \sum_{j=1}^m Y_j\right]=\frac{n}{n+m}\bar{X}_n+\frac{m}{n+m}\bar{Y}_m,\\
\hat{\sigma}^2_0&=\frac{1}{n+m}\left[\sum_{i=1}^n(X_i-\hat{\mu}_0)^2+\sum_{j=1}^m(Y_j-\hat{\mu}_0)^2\right]
\end{align}$$

A estatística do teste da razão de verossimilhanças é

$$\begin{align}
\lambda(\textbf{x,y})&=\frac{L(\hat{\mu}_0,\hat{\sigma}_0^2)}{L(\hat{\mu}_X,\hat{\mu}_Y,\hat{\sigma}^2)}=\frac{\left(\frac{1}{2\pi\hat{\sigma}^2_0}\right)^{\frac{n+m}{2}}e^{-\frac{1}{2\hat{\sigma}^2_0}\left[\sum_{i=1}^n(x_i-\hat{\mu}_0)^2+\sum_{j=1}^m(y_j-\hat{\mu}_0)^2\right]}}{\left(\frac{1}{2\pi\hat{\sigma}^2}\right)^{\frac{n+m}{2}}e^{-\frac{1}{2\hat{\sigma}^2}\left[\sum_{i=1}^n(x_i-\bar{x}_n)^2+\sum_{j=1}^m(y_j-\bar{y}_m)^2\right]}}\\
&=\left(\frac{\hat{\sigma}^2}{\hat{\sigma}_0^2}\right)^{\frac{n+m}{2}}
\end{align}$$
Observe que a estatística do teste da razão de verossimilhanças depende unicamente da razão dos estimadores para a $\sigma^2$. Note ainda que
$$\begin{align}\hat{\sigma}^2_0&=\frac{1}{n+m}\left[\sum_{i=1}^n(x_i-\bar{x}_n+\bar{x}_n-\hat{\mu}_0)^2+\sum_{j=1}^m(y_i-\bar{y}_m+\bar{y}_m-\hat{\mu}_0)^2\right]\\&=\hat{\sigma}^2+n(\bar{x}_n-\hat{\mu}_0)^2+m(\bar{y}_m-\hat{\mu}_0)^2\\&=
\hat{\sigma}^2+n\left[\bar{x}_n\left(1-\frac{n}{n+m}\right)-\frac{m}{n+m}\bar{y}_m\right]^2+m(\bar{y}_m-\hat{\mu}_0)^2\end{align}$$
Fazendo $p=n/(n+m)$, teremos que $\hat{\mu}_0=p\bar{x}_n+(1-p)\bar{y}_m$ e
$$\begin{align}\hat{\sigma}^2_0&=
\hat{\sigma}^2+n(1-p)^2\left(\bar{x}_n-\bar{y}_m\right)^2+mp^2(\bar{x}_n-\bar{y}_m)^2\\&=\hat{\sigma}^2+\frac{nm}{n+m}\left(\bar{x}_n-\bar{y}_m\right)^2\end{align}$$
Portanto,
$$\lambda(\textbf{x,y})=\left(\frac{1}{1+\underbrace{\frac{nm}{n+m}\frac{(\bar{x}_n-\bar{y}_m)^2}{\hat{\sigma}^2}}_{t^2}}\right)^{\frac{n+m}{2}}=\left(\frac{1}{1+t^2}\right)^{\frac{n+m}{2}}$$
e, como já discutido no exemplo anterior, o teste de nível $\alpha$

$$D(\textbf{x,y})=\left\{\begin{array}{ll}1,&\lambda(\textbf{x,y})<k\\0,\hbox{ caso contrário}\end{array}\right.$$ é equivalente ao teste
$$D^*(t)=\left\{\begin{array}{ll}1,&t^2>c\\0,\hbox{ caso contrário}\end{array}\right.$$ é equivalente 
onde $c$ é o quantil $1-\alpha/2$ da distribuição $t$-Student com $n+m-2$ graus de liberdade.
</div>


É fato que a estatística do teste da razão de verossimilhanças sempre vai depender da estatística suficiente. Ou seja, para o TRV de nível $\alpha$ dado por
$$D(\textbf{x})=\left\{\begin{array}{ll}1,&\lambda(\textbf{x})<k,\\ 0,&\lambda(\textbf{x})\geq k.\end{array}\right.$$
e para a estatística suficiente $T$, sempre é possível construir um teste
$$D^*(T)=\left\{\begin{array}{ll}1,&T\in C\\0,&T\notin C \end{array}\right.$$
onde $C$ é um conjunto que satisfaz
$$P(T\in C|\theta_0)=\alpha.$$
Nos exemplos anteriores, $C$ foi encontrado com certa facilidade. Isso não é verdade para a maioria dos casos. É possível remediar esse problema considerando uma distribuição aproximada para $\lambda(\textbf{X})$. Para tanto, considere que $H_0$ é verdadeira e seja $\ell(\theta)=\log L(\theta)$. Podemos aproximar $\ell(\theta_0)$ em torno de $\hat{\theta}$ como segue

$$\ell(\theta_0)\approx \ell(\hat{\theta})+(\theta_0-\hat{\theta})\ell'(\hat{\theta})+\frac{1}{2}(\theta_0-\hat{\theta})^2\ell''(\hat{\theta})$$
e, como $\ell(\hat{\theta})=0$, teremos

$$\ell(\theta_0)-\ell(\hat{\theta})\approx \frac{1}{2}(\theta_0-\hat{\theta})^2\ell''(\hat{\theta})$$
Como $\hat{\theta}$ é consistente, para grandes amostras teremos que 
$$\ell''(\hat{\theta})\approx \ell''(\theta)=-\mathcal{I}_n(\theta)$$
onde $\mathcal{I}_n(\theta)$ é a informação de Fisher associada a amostra de tamanho $n$. Como $\ell(\theta_0)-\ell(\hat{\theta})=\log \lambda(\textbf{X})$, teremos que
$$\log \lambda(\textbf{X})\approx -\frac{1}{2}(\hat{\theta}-\theta_0)^2\mathcal{I}_n(\theta)$$
ou ainda

$$-2\log \lambda(\textbf{X})\approx (\hat{\theta}-\theta_0)^2\mathcal{I}_n(\theta)$$
Por último, como $\hat{\theta}\approx N(\theta_0,\mathcal{I}_n(\theta)^{-1})$, temos que
$$(\hat{\theta}-\theta_0)\sqrt{\mathcal{I}_n(\theta)}\approx N(0,1),$$
logo 

$$-2\log \lambda(\textbf{X})\approx \chi^2_1$$
Como no teste da razão de verossimilhanças a hipótese é rejeitada se $\lambda(\textbf{x})<k$, o teste aproximado é dado por
$$D(\textbf{x})=\left\{\begin{array}{ll}1,&-2\log \lambda(\textbf{x})>c,\\0,&\hbox{ caso contrário}\end{array}\right.,$$
onde $c$ é o quantil $1-\alpha$ da distribuição $\chi^2_1$.

Esse resultado pode ser generalizado, como mostra o seguinte teorema.

<div class='alert alert-success'>
**Teorema.** Seja $q$ o número de parâmetros em $\Theta$. Considere a hipótese $H_0:\theta\in\Theta_0$ e seja $q'$ o número de parâmetros não especificados em $\Theta_0$. Então
$$-2\log \lambda(\textbf{X})\approx \chi^2_{q-q'}.$$

Utilizando -2$\log \lambda(\textbf{X})$ como estatística de teste, rejeitamos $H_0$ se  -2$\log \lambda(\textbf{X})>c$, onde $c$ é o quantil $1-\alpha$ da distribuição $\chi^2_{q-q'}$.
</div>

<div class='alert alert-danger'>
**Importante!** Os parâmetros não especificados no teorema acima dizem respeita àqueles cujo os valores não estão são especificados em $H_0$. Seguem alguns exemplos para fixação:

1. Seja $X_1,\ldots,X_n$ uma amostra aleatória do modelo Poisson($\theta$) (como há um parâmetro, $q=1$). Nesse caso, a distribuição da estatística de teste vai ser calculada considerando algum ponto $\theta_0$ especificado, o que implica que $q'=0$ ( pois todos os parâmetros foram especificados). 

2. Seja $X_1,\ldots,X_n$ uma amostra aleatória do modelo Norma($\theta$,$\sigma^2$) (como há dois parâmetros, $q=2$) e considere um teste sobre a média $\theta$. Nesse caso, a distribuição da estatística de teste vai ser calculada considerando algum ponto $\theta_0$ especificado, o que implica que $q'=1$ ( pois $\sigma^2$ não foi especificado na hipótese nula).

3. Seja $X_1,\ldots,X_n$ uma amostra aleatória do modelo Norma($\mu$,$\sigma^2$) (como há dois parâmetros, $q=2$) e considere a hipótese $H_0:\mu=0,\sigma^2=1$. Nesse caso, $q'=0$, pois os dois parâmetros foram especificados na hipótese.
</div>

<div class='alert alert-info'>
**Exemplo** Seja $X_1,\ldots,X_n$ uma amostra da população Poisson($\theta$). Considere a hipótese $H_0:\theta=\theta_0$. A estatística do teste da razão de verossimilhanças é

$$\begin{align}\lambda(\textbf{x})&=\frac{L(\theta_0)}{L(\hat{\theta})}=\frac{\frac{e^{-n\theta_0}\theta_0^{\sum_{i=1}^n x_i}}{\prod_{i=1}^n x_i!}}{\frac{e^{-n\hat{\theta}}\hat{\theta}^{\sum_{i=1}^n x_i}}{\prod_{i=1}^n x_i!}}=\left(\frac{\theta_0}{\hat{\theta}}\right)^{\sum_{i=1}^n x_i}e^{-n(\theta_0-\hat{\theta})}\end{align}$$
Como $q=1$ e $q'=0$, teremos que
$$-2\log L=-2\sum_{i=1}^n X_i\log\left(\frac{\theta_0}{\hat{\theta}}\right)-n(\theta_0-\hat{\theta})\approx \chi^2_1$$

Considerando um teste (aproximado) de nível 5%, rejeitamos $H_0$ se $-2\log L$ é menor que
```{r}
qchisq(.95,1)
```
</div>

<div class='alert alert-info'>
**Exemplo.** Uma pesquisa foi realizada para comparar a preferência por um novo produto entre dois grupos demográficos distintos: jovens (18-25 anos) e adultos (35-50 anos). Uma amostra aleatória de 200 jovens e 300 adultos foi consultada, e os resultados indicaram que 125 jovens e 175 adultos preferiram o novo produto. Determine se há diferença significativa entre a preferência nos dois grupos.

**Solução**. Considere as seguintes variáveis:
$$X_i=\left\{\begin{array}{ll}1,&\hbox{ se o i-éismo jovem prefere o produto novo }\\0,& \hbox{ caso contrário}\end{array}\right.$$
$$Y_j=\left\{\begin{array}{ll}1,&\hbox{ se o j-éismo adulto prefere o produto novo }\\0,& \hbox{ caso contrário}\end{array}\right.$$
Temos que $X_1,\ldots,X_{200}$ e $Y_1\ldots,Y_{300}$ são duas amostras aleatórias independetes com populações Bernoulli($\theta_X$) e Bernoulli($\theta_Y$), respectivamente. Considere a hipótese $H_0:\theta_X=\theta_Y$. Vamos realizar o teste da razão de verossimilhanças. 


Primeiro, sabemos que os estimadores de máxima verossimilhança para $\theta_X$ e $\theta_Y$ são $\bar{X}$ e $\bar{Y}$, respectivamente. Supondo que $H_0$ é verdadeira, $\theta_X=\theta_Y=\theta$, que pode ser estimado por
$$\hat{\theta}=\frac{\sum_{i=1}^{200}X_i +\sum_{j=1}^{300}Y_j} {500}=\frac{2}{5}\bar{X}+\frac{3}{5}\bar{Y}$$
A estatística do teste da razão de verossimilhanças é dada por
$$\begin{align}\lambda(\textbf{x},\textbf{y})&=\frac{\hat{\theta}^{\sum_{i=1}^{200}x_i+\sum_{j=1}^{300}y_j}(1-\hat{\theta})^{500-\sum_{i=1}^{200}x_i-\sum_{j=1}^{300}y_j}}{\bar{x}^{\sum_{i=1}^{200}x_i}(1-\bar{x})^{200-\sum_{i=1}^{200}x_i}\bar{y}^{\sum_{j=1}^{300}y_j}(1-\bar{y})^{300-\sum_{j=1}^{300}y_j}}\\&=\frac{\hat{\theta}^{500\hat{\theta}}(1-\hat{\theta})^{500(1-\hat{\theta})}}{\bar{x}^{200\bar{x}}(1-\bar{x})^{200(1-\bar{x})}\bar{y}^{300\bar{y}}(1-\bar{y})^{300(1-\bar{y})}}\end{align}$$

Foram observados $\bar{x}=125/200=0,625$ e $\bar{y}=0,583$ e a estimativa de máxima verossimilhança para a proporção quando $H_0$ é verdadeira é $\hat{\theta}=0,6$. A estatística observada é

```{r}
xbar <- 125/200
ybar <- 175/300
theta <- (2/5)*xbar + (3/5)*ybar

logL0 <- 500*theta*log(theta) + 500*(1-theta)*log(1-theta)
logL <- 200*xbar*log(xbar) + 200*(1-xbar)*log(1-xbar) +300*ybar*log(ybar) + 300*(1-ybar)*log(1-ybar)

-2*( logL0 - logL)
```
Como a dimensão de $\Theta$ é 2 e a de $\Theta_0$ é 1, rejeitamos $H_0$ se o valor da estatística é maior que
```{r}
qchisq(.95,1)
```
Como $-2\log\lambda(\textbf{x,y})=0,87 <3,84$, não rejeitamos a hipótese nula, ou seja, não há evidências de diferenças entre os grupos etários em relação à preferência do produto novo. 
</div>


## A função poder e os testes mais poderosos

Já vimos anteriormente que a função poder, dada por
$$\beta(\theta)=P(D=1|\theta)$$
é a probabilidade de rejeição da hipótese nula vista como função do parâmetro.

Se o teste $D$ tem nível de significância $\alpha$, então, quando $\theta\in\Theta_0$, temos que $\beta(\theta)\leq \alpha$.

Sejam $D$ e $D'$ dois testes de hipóteses, ambos com nível de significância $\alpha$ e sejam $\beta(\theta)$ e $\beta'(\theta)$ suas respectivas funções poder. Diremos que $D$ é mais poderoso que $D'$ se
$$\beta(\theta)>\beta'(\theta)$$
para todo $\theta\in\Theta_0^c$. Isso implica que, para qualquer $\theta^*\in\Theta_0^c$, a probabilidade do teste $D$ cometer o erro tipo $II$ se comparada ao teste $D'$.


<div class='alert alert-success'>
**Definição.** Dizemos que  $D$ é um teste uniformemente mais poderoso de nível $\alpha$ se, para qualquer outro teste $D'$ de nível $\alpha$, tem-se que
$$\beta(\theta)>\beta'(\theta)$$
para todo $\theta\in\Theta_0^c$, onde $\beta$ e $\beta'$ são as respectivas funções poder dos testes $D$ e $D'$.
</div>

Até o momento, particionamos o espaço amostral em dois cojuntos, nominalmente $\Theta_0$ e $\Theta_0^c$ e temos estudado problemas nos quais, ao rejeitar $H_0$, aceitamos que $\theta$ é algum ponto dentro de $\Theta_0^c$. Esse problema pode ser reformulado considerando as hipóteses $H_0:\theta\in\Theta_0$ contra $H_1:\theta\in\Theta_0^c$, sendo $H_1$ denominada hipótese alternativa.


Há uma variante deste problema, no qual a hipótese alternativa não é o complementar do espaço paramétrico. Embora tenha aplicação limitada, tal variante nos permite desenvolver resultados importantes em relação aos testes uniformementes mais poderosos. Abaixo, apresentamos o primeiro desses testes. 

<div class='alert alert-success'>
**Definição (Teste de Neyman-Pearson)** Considere a $H_0:\theta=\theta_0$ e suponha que desejamos . A estatística do teste de Neyman-Pearson é dada por 

$$\phi(\textbf{x})=\frac{L(\theta_0)}{L(\theta_1)}.$$
Esse teste é dado por
$$D(\textbf{x})=\left\{\begin{array}{ll}1,&\hbox{ se }\phi(\textbf{x})<k,\\
0,&\hbox{ caso contrário}\end{array}\right.$$
onde $k$ satisfaz $\beta(\theta_0)\leq \alpha.$
</div>

<div class='alert alert-success'>
**Teorema (Lema de Neyman-Pearson)** O teste de Neyman-Pearson de nível $\alpha$ para as hipóteses $H_0:\theta=\theta_0$ contra $H_1:\theta=\theta_1$ é o teste mais poderoso de nível $\alpha$.
</div>

<div class='alert alert-info'>
</div>